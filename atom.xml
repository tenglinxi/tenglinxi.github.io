<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-11-12T11:37:04.094Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Intriguing properties of neural networks</title>
    <link href="http://example.com/2021/11/10/Intriguing%20properties%20of%20neural%20networks%20/"/>
    <id>http://example.com/2021/11/10/Intriguing%20properties%20of%20neural%20networks%20/</id>
    <published>2021-11-10T14:49:29.000Z</published>
    <updated>2021-11-12T11:37:04.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络有趣的性质"><a href="#神经网络有趣的性质" class="headerlink" title="神经网络有趣的性质"></a>神经网络有趣的性质</h1><h2 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0.摘要"></a>0.摘要</h2><p>深度神经网络是具有高度表现力的模型，最近在语音和视觉识别任务上取得了最先进的表现。 虽然他们的表现力是他们成功的原因，但这也使他们学习了可能具有反直觉特性的无法解释的解决方案。 在本文中，我们报告了两个这样的属性。</p><p>首先，根据单元分析 (unit analysis) 的各种方法，我们发现单个高级单元和高级单元的随机线性组合之间没有区别。 它表明在<font color='red'> 神经网络的高层中包含语义信息的是空间，而不是单个单元。</font></p><p>其次，我们发现深度神经网络在很大程度上学习了相当不连续的输入-输出映射。 我们可以<font color='red'>通过应用某种难以察觉的扰动来导致网络对图像进行错误分类</font>，这是通过最大化网络的预测误差来发现的。 此外，这些扰动的特定性质不是学习的随机产物：相同的扰动会导致在数据集的不同子集上训练的不同网络对相同输入进行错误分类。</p><span id="more"></span><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>深度神经网络是强大的学习模型，可以在视觉和语音识别问题上取得出色的性能。 神经网络能够实现高性能，因为它们可以表达由适度数量的大规模并行非线性步骤组成的任意计算。 但是由于结果计算是通过监督学习通过反向传播自动发现的，因此可能难以解释并且可能具有违反直觉的特性。 在本文中，我们讨论了深度神经网络的两个违反直觉的特性。</p><p>第一个属性与单个单元的语义有关。以前的作品通过找到最大程度激活给定单元的输入集来分析各种单元的语义。对单个单元的检查隐含假设最后一个特征层的单元形成一个区分基础，这对于提取语义信息特别有用。相反，我们在第 3 节中展示了 φ(x) 的随机投影在语义上与 φ(x) 的坐标无法区分。这对神经网络跨坐标解开变化因素的猜想提出了质疑。一般来说，似乎是整个激活空间，而不是单个单元，包含了大部分语义信息。 Mikolov 等人最近得出了一个类似但更有力的结论。用单词表示，其中表示单词的向量空间中的各个方向显示出令人惊讶的丰富的关系和类比语义编码。同时，向量表示在空间旋转之前是稳定的，因此向量表示的各个单元不太可能包含语义信息。 </p><p>第二个属性与神经网络相对于其输入的小扰动的稳定性有关。 考虑一个最先进的深度神经网络，它可以很好地概括对象识别任务。 我们期望这样的网络对其输入的小扰动具有健壮性，因为小扰动不能改变图像的对象类别。 然而，我们发现对测试图像应用不可察觉的非随机扰动，可以任意改变网络的预测（见图 5）。 这些扰动是通过优化输入以最大化预测误差来发现的。 我们将如此受干扰的示例称为“<font color='red'>对抗样本”</font>。 </p><p>很自然地期望最小必要扰动的精确配置是反向传播学习不同运行中出现的正常可变性的随机产物。 然而，我们发现对抗样本是相对稳健的，并且由具有不同层数、激活或在训练数据的不同子集上训练的神经网络共享。也就是说，如果我们使用一个神经网络来生成一组对抗样本，我们会发现这些样本对于另一个神经网络来说仍然难以统计，即使它是用不同的超参数训练的，或者最令人惊讶的是，当它在不同的超参数上训练时 一组例子。 </p><p>这些结果表明，通过反向传播学习的深度神经网络具有非直观特征和内在盲点，其结构以非明显方式与数据分布相关联。 </p><h2 id="2-框架"><a href="#2-框架" class="headerlink" title="2.框架"></a>2.框架</h2><h2 id="3-单元：φ-x"><a href="#3-单元：φ-x" class="headerlink" title="3.单元：φ(x)"></a>3.单元：φ(x)</h2><h2 id="4-神经网络中的盲点"><a href="#4-神经网络中的盲点" class="headerlink" title="4.神经网络中的盲点"></a>4.神经网络中的盲点</h2><p>到目前为止，单元级检查方法除了确认有关深度神经网络学习的表示的复杂性的某些直觉之外，几乎没有效用。 网络级检查方法在解释模型做出的分类决策的上下文中非常有用，并且可用于，例如，识别导致给定视觉输入实例正确分类的输入部分 （换句话说，可以使用经过训练的模型进行弱监督定位）。 这种全局分析很有用，因为它们可以让我们更好地理解由训练网络表示的输入到输出映射。 </p><p>一般来说，<font color='red'>神经网络的输出层单元是其输入的高度非线性函数。 当它使用交叉熵损失（使用 Softmax 激活函数）进行训练时，它表示给定输入（以及目前提供的训练集）标签的条件分布。</font>有人认为神经网络的输入和输出单元之间的非线性层的深度堆栈是模型在输入空间上编码非局部泛化先验的一种方式。 换句话说，假设输出单元可以将不显着（并且可能是 non-epsilon）的概率分配给输入空间中在其附近不包含训练示例的区域。 例如，这些区域可以表示来自不同视点的相同对象，它们相对较远（在像素空间中），但仍然共享原始输入的标签和统计结构。 </p><p>在这样的论点中隐含着<font color='red'>局部泛化</font>——在非常接近训练示例的地方——按预期工作。 特别的，对于给定训练输入 x 附近足够小的半径 ε &gt; 0，x + r 满足 ||r|| &lt; ε 将被模型分配高概率的正确类别。 </p><p>从某种意义上说，我们所描述的是一种以有效的方式（通过优化）遍历由网络表示的流形并在输入空间中找到对抗样本的方法。 对抗样本代表流形中的低概率（高维）“口袋”，通过简单地围绕给定示例对输入进行简单随机采样，很难有效地找到这些“口袋”。 已经有各种最新的计算机视觉模型在训练期间采用输入变形来提高模型的鲁棒性和收敛速度。 然而，对于给定的例子，这些变形在统计上是低效的：它们高度相关，并且在整个模型训练中来自相同的分布。 我们提出了一种方案，以利用模型及其在对训练数据周围的局部空间进行建模的缺陷的方式使该过程具有适应性。 </p><p>我们明确地将硬负挖掘与硬负挖掘联系起来，因为它在精神上很接近：在计算机视觉中，硬负挖掘包括识别模型给出的低概率的训练集示例（或其部分），但应该 而是高概率。 然后改变训练集分布以强调这种硬负，并执行进一步的模型训练。 正如将要描述的，这项工作中提出的优化问题也可以以建设性的方式使用，类似于硬负挖掘原则。 </p><h3 id="4-1正式描述"><a href="#4-1正式描述" class="headerlink" title="4.1正式描述"></a>4.1正式描述</h3><h3 id="4-2实验结果"><a href="#4-2实验结果" class="headerlink" title="4.2实验结果"></a>4.2实验结果</h3><h3 id="4-3不稳定性的谱分析"><a href="#4-3不稳定性的谱分析" class="headerlink" title="4.3不稳定性的谱分析"></a>4.3不稳定性的谱分析</h3><h2 id="5-论述"><a href="#5-论述" class="headerlink" title="5.论述"></a>5.论述</h2><p>我们证明了深度神经网络在单个单元的语义及其不连续性方面都具有反直觉的特性。对抗性否定的存在似乎与神经网络实现高泛化性能的能力相矛盾。 确实，如果网络可以很好地泛化，它怎么会被这些与常规示例无法区分的具有对抗性的负样本混淆？ 可能的解释是对抗性负集的概率极低，因此在测试集中从未（或很少）观察到，但它是密集的（很像有理数），因此它几乎可以在每个 测试用例。 然而，我们并没有深入了解出现具有对抗性的负样本的出现的频率，因此这个问题应该在未来的研究中得到解决。 </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>首次提出了对抗样本的概念</li><li>提出了对抗样本具有迁移性</li></ul><h1 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h1><h2 id="hard-negative-hard-negative"><a href="#hard-negative-hard-negative" class="headerlink" title="hard-negative / hard negative"></a>hard-negative / hard negative</h2><h1 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h1><ul><li>inspection n. 检查</li><li>presumably 大概</li><li>proximity n. 接近;(时间或空间)邻近;靠近</li><li>traverse vt.横过;横越;穿过;横渡</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;神经网络有趣的性质&quot;&gt;&lt;a href=&quot;#神经网络有趣的性质&quot; class=&quot;headerlink&quot; title=&quot;神经网络有趣的性质&quot;&gt;&lt;/a&gt;神经网络有趣的性质&lt;/h1&gt;&lt;h2 id=&quot;0-摘要&quot;&gt;&lt;a href=&quot;#0-摘要&quot; class=&quot;headerlink&quot; title=&quot;0.摘要&quot;&gt;&lt;/a&gt;0.摘要&lt;/h2&gt;&lt;p&gt;深度神经网络是具有高度表现力的模型，最近在语音和视觉识别任务上取得了最先进的表现。 虽然他们的表现力是他们成功的原因，但这也使他们学习了可能具有反直觉特性的无法解释的解决方案。 在本文中，我们报告了两个这样的属性。&lt;/p&gt;
&lt;p&gt;首先，根据单元分析 (unit analysis) 的各种方法，我们发现单个高级单元和高级单元的随机线性组合之间没有区别。 它表明在&lt;font color=&#39;red&#39;&gt; 神经网络的高层中包含语义信息的是空间，而不是单个单元。&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;其次，我们发现深度神经网络在很大程度上学习了相当不连续的输入-输出映射。 我们可以&lt;font color=&#39;red&#39;&gt;通过应用某种难以察觉的扰动来导致网络对图像进行错误分类&lt;/font&gt;，这是通过最大化网络的预测误差来发现的。 此外，这些扰动的特定性质不是学习的随机产物：相同的扰动会导致在数据集的不同子集上训练的不同网络对相同输入进行错误分类。&lt;/p&gt;</summary>
    
    
    
    
    <category term="对抗样本" scheme="http://example.com/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2021/11/10/hello-world/"/>
    <id>http://example.com/2021/11/10/hello-world/</id>
    <published>2021-11-10T13:49:56.066Z</published>
    <updated>2021-11-10T13:49:56.066Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
